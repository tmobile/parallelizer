// Copyright (c) 2020 Kevin L. Mitchell
//
// Licensed under the Apache License, Version 2.0 (the "License"); you
// may not use this file except in compliance with the License.  You
// may obtain a copy of the License at
//
//      http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
// implied.  See the License for the specific language governing
// permissions and limitations under the License.

package parallelizer

import (
	"container/list"
	"runtime"
	"sync"
)

// Sizes of the submit, work, and results channels.
const (
	SubmitBuffer  = 100
	WorkBuffer    = 100
	ResultsBuffer = 100
)

// managerItem is a structure for communicating to the manager.  When
// coming from parallelWorker.Call, it contains data to work; when
// coming from parallelWorker.Wait, it contains an instruction for the
// manager to exit.  From workers, it either contains a result or a
// flag that the worker has exited.
type managerItem struct {
	data interface{} // Data to work or result to integrate
	done bool        // If true, exit or has exited
}

// parallelWorker is an implementation of the Worker interface that
// operates in an parallel fashion; that is, there are worker
// goroutines involved, along with a manager goroutine.  This is the
// worker used to enable parallelization.
type parallelWorker struct {
	sync.Mutex
	state   workerState      // State of the worker
	runner  Runner           // The runner to be invoked by the workers
	workers int              // Total number of workers to use
	manager *parallelManager // The manager for the workers
	gonner  *sync.Once       // A once invocation for getting the result
	result  interface{}      // The result from the work
}

// NewParallelWorker constructs a worker utilizing a pool of worker
// goroutines.  Parallel workers can receive Call and Wait invocations
// from almost any goroutine (with the exception of the goroutines
// running the Runner.Run and Runner.Integrate methods; the latter,
// however, receives an alternate implementation of Worker which is
// safe to use).  A parallel worker is initialized with a desired
// number of worker goroutines; if that number is less than or equal
// to 0, the number of CPU cores detected by the go runtime will be
// used instead.  (See runtime.NumCPU.)
func NewParallelWorker(runner Runner, workers int) Worker {
	// Normalize workers
	if workers <= 0 {
		workers = runtime.NumCPU()
	}

	return &parallelWorker{
		runner:  runner,
		workers: workers,
		gonner:  &sync.Once{},
	}
}

// startManager initializes the manager and sets it running.
func (w *parallelWorker) startManager() {
	// Construct the manager
	w.manager = &parallelManager{
		worker:  w,
		queue:   &list.List{},
		submit:  make(chan *managerItem, SubmitBuffer),
		work:    make(chan interface{}, WorkBuffer),
		results: make(chan *managerItem, ResultsBuffer),
		done:    make(chan bool, 1),
	}

	// And start it
	go w.manager.manager()
}

// getResult is a helper for Wait to retrieve the result.  It's called
// with parallelWorker.gonner to ensure that it only gets called once.
func (w *parallelWorker) getResult() {
	w.result = w.runner.Result()
	w.state = workerResult
}

// Call is the method used to submit data to be worked in a call to
// the Runner.Run method.  It may return an error if the parallelizer
// has been shut down through a call to Wait.
func (w *parallelWorker) Call(data interface{}) error {
	// Lock the mutex
	w.Lock()
	defer w.Unlock()

	switch w.state {
	case workerNew: // Need to start the manager
		w.startManager()
		w.state = workerRunning

	case workerClosed, workerResult: // Oh, we're closed
		return ErrWorkerClosed
	}

	// OK, submit the data
	w.manager.submit <- &managerItem{data: data}

	return nil
}

// Wait is called to shut down the worker and return the final result;
// it will block the worker until all data has been processed and all
// worker goroutines have stopped.  Note that the final result,
// generated by Runner.Result, is saved by Worker to satisfy later
// calls to Wait.  If Wait is called before any calls to Call, the
// worker will go straight to a stopped state, and no further Call
// calls may be made; no error will be returned in that case.
func (w *parallelWorker) Wait() (interface{}, error) {
	// Lock the mutex
	w.Lock()

	switch w.state {
	case workerNew: // Haven't even started yet
		w.state = workerClosed
		w.Unlock()
		w.gonner.Do(w.getResult)

	case workerRunning: // Signal to die, wait for done
		w.state = workerClosed
		w.Unlock()
		w.manager.submit <- &managerItem{done: true}
		<-w.manager.done
		w.manager = nil // clear to break cycle
		w.gonner.Do(w.getResult)

	case workerClosed: // Closed, waiting for result
		w.Unlock()
		w.gonner.Do(w.getResult)

	case workerResult: // Have result, just need to unlock
		w.Unlock()
	}

	return w.result, nil
}

// parallelManager is an implementation of the Worker interface for
// use by the manager goroutine; it is passed to the Integrate methods
// to allow them to make additional calls to Call, and so allows
// alternate implementations of the Worker methods.
type parallelManager struct {
	worker  *parallelWorker   // The worker we're managing for
	exiting bool              // A flag used by the manager's main loop
	count   int               // The number of running workers
	queue   *list.List        // A queue of submitted work items
	submit  chan *managerItem // A channel for submitting run requests
	work    chan interface{}  // A channel for sending work to the workers
	results chan *managerItem // A channel for workers to return results
	done    chan bool         // A channel to tell Wait the manager is done
}

// workRunner is the actual worker routine.
func (w *parallelManager) workRunner(work <-chan interface{}) {
	// Make sure to signal manager when we exit
	defer func() { w.results <- &managerItem{done: true} }()

	// Do the work
	for data := range work {
		result := w.worker.runner.Run(data)
		w.results <- &managerItem{data: result}
	}
}

// startWorkers handles ensuring that the requested number of workers
// are available.
func (w *parallelManager) startWorkers() {
	for w.count < w.worker.workers {
		go w.workRunner(w.work)
		w.count++
	}
}

// receiveWork handles the receipt of a work item, submitted by
// parallelWorker.Call or parallelWorker.Wait.
func (w *parallelManager) receiveWork(newItem *managerItem) {
	// Is it an exit instruction?
	if newItem.done {
		w.exiting = true
		return
	}

	// Push the work item onto the queue
	w.queue.PushBack(newItem.data)
}

// receiveResult handles the receipt of a result from a worker.  It
// will call the Runner.Integrate method with that result.
func (w *parallelManager) receiveResult(result *managerItem) {
	// Is it an indication that the worker exited?
	if result.done {
		w.count--
		return
	}

	// Integrate the result
	w.worker.runner.Integrate(w, result.data)
}

// managerSelect performs an appropriate select call to coordinate the
// various channels used by the manager.
func (w *parallelManager) managerSelect() bool {
	selectors := []selector{}

	// If there are any elements in the queue, add a selector to
	// send the top one to the workers
	if w.queue.Len() > 0 {
		selectors = append(selectors, selectSend(w.work, w.queue.Front().Value, func() {
			w.queue.Remove(w.queue.Front())
		}))
	}

	// If we're not exiting, add a selector to receive from the
	// submit channel
	if !w.exiting {
		selectors = append(selectors, selectRecv(w.submit, func(value interface{}, ok bool) {
			w.receiveWork(value.(*managerItem))
		}))
	}

	// If we still have workers, add a selector to receive results
	// from the results channel
	if w.count > 0 {
		selectors = append(selectors, selectRecv(w.results, func(value interface{}, ok bool) {
			w.receiveResult(value.(*managerItem))
		}))
	}

	// If we have no selectors, do nothing
	if len(selectors) <= 0 {
		return false
	}

	// Do the select
	doSelect(selectors)

	return true
}

// manager is the core loop of the manager.  It runs until there is no
// more work to send and no more results to collect.
func (w *parallelManager) manager() {
	defer func() { w.done <- true }()

	for w.managerSelect() {
		if w.exiting && w.queue.Len() <= 0 && w.work != nil {
			// Manager's been told to exit and we've
			// cleared the queue; close the work channel
			// so the workers can exit
			close(w.work)
			w.work = nil
		} else if w.work != nil {
			// Make sure we have the requested number of
			// workers
			w.startWorkers()
		}
	}
}

// Call is the method used to submit data to be worked in a call to
// the Runner.Run method.  It may return an error if the parallelizer
// has been shut down through a call to Wait.
func (w *parallelManager) Call(data interface{}) error {
	// Add the data to the queue
	w.queue.PushBack(data)

	return nil
}

// Wait is called to shut down the worker and return the final result;
// it will block the worker until all data has been processed and all
// worker goroutines have stopped.  Note that the final result,
// generated by Runner.Result, is saved by Worker to satisfy later
// calls to Wait.  If Wait is called before any calls to Call, the
// worker will go straight to a stopped state, and no further Call
// calls may be made; no error will be returned in that case.
func (w *parallelManager) Wait() (interface{}, error) {
	return nil, ErrWouldDeadlock
}
